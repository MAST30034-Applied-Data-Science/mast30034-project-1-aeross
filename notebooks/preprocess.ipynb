{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "from readparquet import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:21:20 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 192.168.207.1 instead (on interface eth1)\n",
      "22/08/01 17:21:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:21:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/notebooks/preprocess.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# let's import all the green taxi data for the past 6 months\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m sdf_g1 \u001b[39m=\u001b[39m read_parquet(\u001b[39m'\u001b[39;49m\u001b[39m../data/raw/green_tripdata_2021-11.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000001vscode-remote?line=2'>3</a>\u001b[0m sdf_g2 \u001b[39m=\u001b[39m read_parquet(\u001b[39m'\u001b[39m\u001b[39m../data/raw/green_tripdata_2021-12.parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000001vscode-remote?line=3'>4</a>\u001b[0m sdf_g3 \u001b[39m=\u001b[39m read_parquet(\u001b[39m'\u001b[39m\u001b[39m../data/raw/green_tripdata_2022-01.parquet\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/notebooks/../scripts/readparquet.py:11\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      9\u001b[0m \u001b[39m# Create a spark session (which will run spark jobs)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 11\u001b[0m     SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mMAST30034 Tutorial 1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.repl.eagerEval.enabled\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m     13\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.parquet.cacheMetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[1;32m    200\u001b[0m         sparkHome,\n\u001b[1;32m    201\u001b[0m         pyFiles,\n\u001b[1;32m    202\u001b[0m         environment,\n\u001b[1;32m    203\u001b[0m         batchSize,\n\u001b[1;32m    204\u001b[0m         serializer,\n\u001b[1;32m    205\u001b[0m         conf,\n\u001b[1;32m    206\u001b[0m         jsc,\n\u001b[1;32m    207\u001b[0m         profiler_cls,\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    283\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1584\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1576\u001b[0m args_command \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1577\u001b[0m     [get_command_part(arg, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m new_args])\n\u001b[1;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fqn)\n\u001b[1;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's import all the green taxi data for the past 6 months\n",
    "sdf_g1 = read_parquet('../data/raw/green_tripdata_2021-11.parquet')\n",
    "sdf_g2 = read_parquet('../data/raw/green_tripdata_2021-12.parquet')\n",
    "sdf_g3 = read_parquet('../data/raw/green_tripdata_2022-01.parquet')\n",
    "sdf_g4 = read_parquet('../data/raw/green_tripdata_2022-02.parquet')\n",
    "sdf_g5 = read_parquet('../data/raw/green_tripdata_2022-03.parquet')\n",
    "sdf_g6 = read_parquet('../data/raw/green_tripdata_2022-04.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   VendorID               731 non-null    int64         \n",
      " 1   lpep_pickup_datetime   731 non-null    datetime64[ns]\n",
      " 2   lpep_dropoff_datetime  731 non-null    datetime64[ns]\n",
      " 3   store_and_fwd_flag     644 non-null    object        \n",
      " 4   RatecodeID             644 non-null    float64       \n",
      " 5   PULocationID           731 non-null    int64         \n",
      " 6   DOLocationID           731 non-null    int64         \n",
      " 7   passenger_count        644 non-null    float64       \n",
      " 8   trip_distance          731 non-null    float64       \n",
      " 9   fare_amount            731 non-null    float64       \n",
      " 10  extra                  731 non-null    float64       \n",
      " 11  mta_tax                731 non-null    float64       \n",
      " 12  tip_amount             731 non-null    float64       \n",
      " 13  tolls_amount           731 non-null    float64       \n",
      " 14  ehail_fee              0 non-null      float64       \n",
      " 15  improvement_surcharge  731 non-null    float64       \n",
      " 16  total_amount           731 non-null    float64       \n",
      " 17  payment_type           644 non-null    float64       \n",
      " 18  trip_type              644 non-null    float64       \n",
      " 19  congestion_surcharge   644 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(14), int64(3), object(1)\n",
      "memory usage: 114.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-30 23:10:48</td>\n",
       "      <td>2022-04-30 23:25:14</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>181</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-06 19:33:42</td>\n",
       "      <td>2022-04-06 19:39:17</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-16 09:49:25</td>\n",
       "      <td>2022-04-16 09:49:45</td>\n",
       "      <td>N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-17 02:11:57</td>\n",
       "      <td>2022-04-17 02:16:12</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-20 20:46:01</td>\n",
       "      <td>2022-04-20 20:50:53</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-12 01:03:00</td>\n",
       "      <td>2022-04-12 01:24:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196</td>\n",
       "      <td>129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.31</td>\n",
       "      <td>13.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "626         1  2022-04-30 23:10:48   2022-04-30 23:25:14                  N   \n",
       "102         2  2022-04-06 19:33:42   2022-04-06 19:39:17                  N   \n",
       "320         1  2022-04-16 09:49:25   2022-04-16 09:49:45                  N   \n",
       "335         2  2022-04-17 02:11:57   2022-04-17 02:16:12                  N   \n",
       "398         2  2022-04-20 20:46:01   2022-04-20 20:50:53                  N   \n",
       "682         2  2022-04-12 01:03:00   2022-04-12 01:24:00               None   \n",
       "\n",
       "     RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "626         1.0           181            65              1.0           0.00   \n",
       "102         1.0           134           134              5.0           0.97   \n",
       "320         5.0            80            80              1.0           0.00   \n",
       "335         1.0             7           146              1.0           1.17   \n",
       "398         1.0            75            75              1.0           0.48   \n",
       "682         NaN           196           129              NaN           2.31   \n",
       "\n",
       "     fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "626        19.20    0.0      0.5        0.00           0.0        NaN   \n",
       "102         5.50    0.0      0.5        0.94           0.0        NaN   \n",
       "320        35.00    0.0      0.0        0.00           0.0        NaN   \n",
       "335         5.50    0.0      0.5        0.00           0.0        NaN   \n",
       "398         5.00    0.0      0.5        0.00           0.0        NaN   \n",
       "682        13.11    0.0      0.0        1.49           0.0        NaN   \n",
       "\n",
       "     improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "626                    0.3         20.00           1.0        1.0   \n",
       "102                    0.3          7.24           1.0        1.0   \n",
       "320                    0.0         35.00           1.0        2.0   \n",
       "335                    0.3          6.30           2.0        1.0   \n",
       "398                    0.3          5.80           2.0        1.0   \n",
       "682                    0.3         14.90           NaN        NaN   \n",
       "\n",
       "     congestion_surcharge  \n",
       "626                   0.0  \n",
       "102                   0.0  \n",
       "320                   0.0  \n",
       "335                   0.0  \n",
       "398                   0.0  \n",
       "682                   NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see what features this data have, and how this data *roughly* looks like\n",
    "df_g6 = sdf_g6.sample(0.01, seed=0).toPandas()\n",
    "df_g6.info()\n",
    "df_g6.sample(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdf_g1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/notebooks/preprocess.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m feats \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtrip_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrip_distance\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPULocationID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDOLocationID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRateCodeID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfare_amount\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mextra\u001b[39m\u001b[39m\"\u001b[39m, \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtip_amount\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtolls_amount\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcongestion_surcharge\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMTA_tax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtotal_amount\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlpep_dropoff_datetime\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlpep_pickup_datetime\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=4'>5</a>\u001b[0m \u001b[39m# improvement_surcharge is excluded because all rides charge the same $0.3.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=6'>7</a>\u001b[0m sdf_g1 \u001b[39m=\u001b[39m sdf_g1\u001b[39m.\u001b[39mselect(col(feats[\u001b[39m0\u001b[39m]), col(feats[\u001b[39m1\u001b[39m]), col(feats[\u001b[39m2\u001b[39m]), col(feats[\u001b[39m3\u001b[39m]), col(feats[\u001b[39m4\u001b[39m]), col(feats[\u001b[39m5\u001b[39m]), \\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=7'>8</a>\u001b[0m     col(feats[\u001b[39m6\u001b[39m]), col(feats[\u001b[39m7\u001b[39m]), col(feats[\u001b[39m8\u001b[39m]), col(feats[\u001b[39m9\u001b[39m]), col(feats[\u001b[39m10\u001b[39m]), col(feats[\u001b[39m11\u001b[39m]), col(feats[\u001b[39m12\u001b[39m]), col(feats[\u001b[39m13\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=8'>9</a>\u001b[0m sdf_g2 \u001b[39m=\u001b[39m sdf_g2\u001b[39m.\u001b[39mselect(col(feats[\u001b[39m0\u001b[39m]), col(feats[\u001b[39m1\u001b[39m]), col(feats[\u001b[39m2\u001b[39m]), col(feats[\u001b[39m3\u001b[39m]), col(feats[\u001b[39m4\u001b[39m]), col(feats[\u001b[39m5\u001b[39m]), \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=9'>10</a>\u001b[0m     col(feats[\u001b[39m6\u001b[39m]), col(feats[\u001b[39m7\u001b[39m]), col(feats[\u001b[39m8\u001b[39m]), col(feats[\u001b[39m9\u001b[39m]), col(feats[\u001b[39m10\u001b[39m]), col(feats[\u001b[39m11\u001b[39m]), col(feats[\u001b[39m12\u001b[39m]), col(feats[\u001b[39m13\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=10'>11</a>\u001b[0m sdf_g3 \u001b[39m=\u001b[39m sdf_g3\u001b[39m.\u001b[39mselect(col(feats[\u001b[39m0\u001b[39m]), col(feats[\u001b[39m1\u001b[39m]), col(feats[\u001b[39m2\u001b[39m]), col(feats[\u001b[39m3\u001b[39m]), col(feats[\u001b[39m4\u001b[39m]), col(feats[\u001b[39m5\u001b[39m]), \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000003vscode-remote?line=11'>12</a>\u001b[0m     col(feats[\u001b[39m6\u001b[39m]), col(feats[\u001b[39m7\u001b[39m]), col(feats[\u001b[39m8\u001b[39m]), col(feats[\u001b[39m9\u001b[39m]), col(feats[\u001b[39m10\u001b[39m]), col(feats[\u001b[39m11\u001b[39m]), col(feats[\u001b[39m12\u001b[39m]), col(feats[\u001b[39m13\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdf_g1' is not defined"
     ]
    }
   ],
   "source": [
    "# all the necessary features (I ditch the ones that I absolutely sure aren't needed, and keep the ones I'm \n",
    "# still somewhat unsure whether they are needed or not, and do some feature engineering later)\n",
    "feats = [\"trip_type\", \"trip_distance\", \"PULocationID\", \"DOLocationID\", \"RateCodeID\", \"fare_amount\", \"extra\", \\\n",
    "    \"tip_amount\", \"tolls_amount\", \"congestion_surcharge\", \"MTA_tax\", \"total_amount\", \"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"]\n",
    "# improvement_surcharge is excluded because all rides charge the same $0.3.\n",
    "\n",
    "sdf_g1 = sdf_g1.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_g2 = sdf_g2.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_g3 = sdf_g3.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_g4 = sdf_g4.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_g5 = sdf_g5.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_g6 = sdf_g6.select(col(feats[0]), col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), \\\n",
    "    col(feats[6]), col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "\n",
    "# I also need data for the length of each trip, and that is acquired by calculating\n",
    "# the difference between lpep_pickup_datetime and lpep_dropoff_datetime.\n",
    "time_diff = unix_timestamp(\"lpep_dropoff_datetime\") - unix_timestamp(\"lpep_pickup_datetime\")\n",
    "\n",
    "sdf_g = sdf_g1.union(sdf_g2).union(sdf_g3).union(sdf_g4).union(sdf_g5).union(sdf_g6)\n",
    "sdf_g = sdf_g.withColumn(\"trip_duration\", time_diff)\n",
    "\n",
    "# only need the trip duration, so these ones can be dropped off\n",
    "sdf_g = sdf_g.drop(\"lpep_dropoff_datetime\").drop(\"lpep_pickup_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save this as a new parquet file\n",
    "sdf_g.write.mode(\"overwrite\").parquet(\"../data/curated/green\")\n",
    "# to copy what tute1 said: \"Your directory might look a bit funky. Don't worry, just leave it as is,\n",
    "# (we don't have time to cover everything about Spark unfortunately) and you can just read in the directory as is.\"\n",
    "# I TAKE YOUR WORD FOR THIS, OKAY!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same thing (yes, same exact thing) for the yellow taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:21:42 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 192.168.207.1 instead (on interface eth1)\n",
      "22/08/01 17:21:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:21:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35795 entries, 0 to 35794\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   VendorID               35795 non-null  int64         \n",
      " 1   tpep_pickup_datetime   35795 non-null  datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  35795 non-null  datetime64[ns]\n",
      " 3   passenger_count        34612 non-null  float64       \n",
      " 4   trip_distance          35795 non-null  float64       \n",
      " 5   RatecodeID             34612 non-null  float64       \n",
      " 6   store_and_fwd_flag     34612 non-null  object        \n",
      " 7   PULocationID           35795 non-null  int64         \n",
      " 8   DOLocationID           35795 non-null  int64         \n",
      " 9   payment_type           35795 non-null  int64         \n",
      " 10  fare_amount            35795 non-null  float64       \n",
      " 11  extra                  35795 non-null  float64       \n",
      " 12  mta_tax                35795 non-null  float64       \n",
      " 13  tip_amount             35795 non-null  float64       \n",
      " 14  tolls_amount           35795 non-null  float64       \n",
      " 15  improvement_surcharge  35795 non-null  float64       \n",
      " 16  total_amount           35795 non-null  float64       \n",
      " 17  congestion_surcharge   34612 non-null  float64       \n",
      " 18  airport_fee            34612 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(12), int64(4), object(1)\n",
      "memory usage: 5.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13111</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-12 20:35:28</td>\n",
       "      <td>2022-04-12 20:41:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31488</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-29 00:44:13</td>\n",
       "      <td>2022-04-29 00:55:33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31591</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-29 02:57:53</td>\n",
       "      <td>2022-04-29 03:29:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>20.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>29.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-05 04:52:37</td>\n",
       "      <td>2022-04-05 04:56:20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.62</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13075</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-04-12 20:11:43</td>\n",
       "      <td>2022-04-12 20:23:09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35099</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-12 09:03:00</td>\n",
       "      <td>2022-04-12 09:12:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>238</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "13111         2  2022-04-12 20:35:28   2022-04-12 20:41:00              1.0   \n",
       "31488         1  2022-04-29 00:44:13   2022-04-29 00:55:33              0.0   \n",
       "31591         1  2022-04-29 02:57:53   2022-04-29 03:29:38              1.0   \n",
       "4233          2  2022-04-05 04:52:37   2022-04-05 04:56:20              3.0   \n",
       "13075         1  2022-04-12 20:11:43   2022-04-12 20:23:09              1.0   \n",
       "35099         2  2022-04-12 09:03:00   2022-04-12 09:12:00              NaN   \n",
       "\n",
       "       trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "13111           0.90         1.0                  N            90   \n",
       "31488           1.30         1.0                  N           239   \n",
       "31591           3.70         1.0                  N           236   \n",
       "4233            1.12         1.0                  N           162   \n",
       "13075           1.80         1.0                  N           151   \n",
       "35099           2.36         NaN               None           238   \n",
       "\n",
       "       DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "13111           113             1         5.50    0.0      0.5        2.20   \n",
       "31488           142             1         9.00    2.5      0.5        2.70   \n",
       "31591           113             1        20.50    3.5      0.5        4.95   \n",
       "4233            137             1         5.00    1.0      0.5        2.32   \n",
       "13075           142             2         8.50    2.5      0.5        0.00   \n",
       "35099            48             0        10.79    0.0      0.5        2.00   \n",
       "\n",
       "       tolls_amount  improvement_surcharge  total_amount  \\\n",
       "13111           0.0                    0.3         11.00   \n",
       "31488           0.0                    0.3         15.00   \n",
       "31591           0.0                    0.3         29.75   \n",
       "4233            0.0                    0.3         11.62   \n",
       "13075           0.0                    0.3         11.80   \n",
       "35099           0.0                    0.3         16.09   \n",
       "\n",
       "       congestion_surcharge  airport_fee  \n",
       "13111                   2.5          0.0  \n",
       "31488                   2.5          0.0  \n",
       "31591                   2.5          0.0  \n",
       "4233                    2.5          0.0  \n",
       "13075                   2.5          0.0  \n",
       "35099                   NaN          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_y1 = read_parquet('../data/raw/yellow_tripdata_2021-11.parquet')\n",
    "sdf_y2 = read_parquet('../data/raw/yellow_tripdata_2021-12.parquet')\n",
    "sdf_y3 = read_parquet('../data/raw/yellow_tripdata_2022-01.parquet')\n",
    "sdf_y4 = read_parquet('../data/raw/yellow_tripdata_2022-02.parquet')\n",
    "sdf_y5 = read_parquet('../data/raw/yellow_tripdata_2022-03.parquet')\n",
    "sdf_y6 = read_parquet('../data/raw/yellow_tripdata_2022-04.parquet')\n",
    "df_y6 = sdf_y6.sample(0.01, seed=0).toPandas()\n",
    "df_y6.info()\n",
    "df_y6.sample(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trip_type', 'trip_distance', 'PULocationID', 'DOLocationID', 'RateCodeID', 'fare_amount', 'extra', 'tip_amount', 'tolls_amount', 'congestion_surcharge', 'MTA_tax', 'total_amount', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (42 + 6) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:23:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "22/08/01 17:23:15 WARN Utils: Suppressing exception in catch: GC overhead limit exceeded\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat sun.text.resources.FormatData.getContents(FormatData.java:122)\n",
      "\tat sun.util.resources.ParallelListResourceBundle.loadLookupTablesIfNecessary(ParallelListResourceBundle.java:168)\n",
      "\tat sun.util.resources.ParallelListResourceBundle.handleKeySet(ParallelListResourceBundle.java:134)\n",
      "\tat sun.util.resources.ParallelListResourceBundle.keySet(ParallelListResourceBundle.java:143)\n",
      "\tat sun.util.resources.ParallelListResourceBundle.containsKey(ParallelListResourceBundle.java:129)\n",
      "\tat sun.util.resources.ParallelListResourceBundle$KeySet.contains(ParallelListResourceBundle.java:208)\n",
      "\tat sun.util.resources.ParallelListResourceBundle.containsKey(ParallelListResourceBundle.java:129)\n",
      "\tat sun.util.locale.provider.LocaleResources.getDecimalFormatSymbolsData(LocaleResources.java:182)\n",
      "\tat java.text.DecimalFormatSymbols.initialize(DecimalFormatSymbols.java:616)\n",
      "\tat java.text.DecimalFormatSymbols.<init>(DecimalFormatSymbols.java:113)\n",
      "\tat sun.util.locale.provider.DecimalFormatSymbolsProviderImpl.getInstance(DecimalFormatSymbolsProviderImpl.java:85)\n",
      "\tat java.text.DecimalFormatSymbols.getInstance(DecimalFormatSymbols.java:180)\n",
      "\tat java.util.Formatter.getZero(Formatter.java:2283)\n",
      "\tat java.util.Formatter.<init>(Formatter.java:1892)\n",
      "\tat java.util.Formatter.<init>(Formatter.java:1914)\n",
      "\tat java.lang.String.format(String.java:2940)\n",
      "\tat org.apache.parquet.bytes.CapacityByteArrayOutputStream.<init>(CapacityByteArrayOutputStream.java:153)\n",
      "\tat org.apache.parquet.column.values.plain.PlainValuesWriter.<init>(PlainValuesWriter.java:47)\n",
      "\tat org.apache.parquet.column.values.dictionary.DictionaryValuesWriter$PlainLongDictionaryValuesWriter.toDictPageAndClose(DictionaryValuesWriter.java:362)\n",
      "\tat org.apache.parquet.column.values.fallback.FallbackValuesWriter.toDictPageAndClose(FallbackValuesWriter.java:109)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriterBase.finalizeColumnChunk(ColumnWriterBase.java:306)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreBase.flush(ColumnWriteStoreBase.java:188)\n",
      "\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:185)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:124)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:164)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:335)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3152/1965669991.apply$mcV$sp(Unknown Source)\n",
      "22/08/01 17:23:15 ERROR Executor: Exception in task 27.0 in stage 7.0 (TID 41)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "22/08/01 17:23:15 WARN TaskSetManager: Lost task 27.0 in stage 7.0 (TID 41) (192.168.207.1 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\n",
      "22/08/01 17:23:15 ERROR TaskSetManager: Task 27 in stage 7.0 failed 1 times; aborting job\n",
      "22/08/01 17:23:15 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 27.0 in stage 7.0 (TID 41),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "22/08/01 17:23:15 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:115)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/01 17:23:15 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:115)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/01 17:23:15 ERROR FileFormatWriter: Aborting job 177d9ebd-37f3-46e1-b591-df85889b1f1d.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 7.0 failed 1 times, most recent failure: Lost task 27.0 in stage 7.0 (TID 41) (192.168.207.1 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.parquet.io.api.Binary.fromReusedByteArray(Binary.java:344)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10(ParquetWriteSupport.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$10$adapted(ParquetWriteSupport.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3181/1431369788.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3189/755744063.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:163)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$Lambda$3186/189920343.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:474)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:153)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:55)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:181)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3151/1467771943.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3145/835028869.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1482/834459788.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "22/08/01 17:23:15 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:115)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/08/01 17:23:15 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:115)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:553)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (42 + 5) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/01 17:23:16 WARN FileOutputCommitter: Could not delete file:/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/data/curated/yellow/_temporary/0/_temporary/attempt_20220801172202882859211111092495_0007_m_000035_49\n",
      "22/08/01 17:23:16 WARN FileOutputCommitter: Could not delete file:/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/data/curated/yellow/_temporary/0/_temporary/attempt_202208011722027309268148982802965_0007_m_000003_17\n",
      "22/08/01 17:23:16 WARN FileOutputCommitter: Could not delete file:/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/data/curated/yellow/_temporary/0/_temporary/attempt_2022080117220244287902655249111_0007_m_000043_57\n",
      "22/08/01 17:23:16 ERROR FileFormatWriter: Job job_2022080117220244287902655249111_0007 aborted.\n",
      "22/08/01 17:23:16 ERROR FileFormatWriter: Job job_202208011722027309268148982802965_0007 aborted.\n",
      "22/08/01 17:23:16 ERROR FileFormatWriter: Job job_20220801172202882859211111092495_0007 aborted.\n",
      "22/08/01 17:23:16 WARN FileOutputCommitter: Could not delete file:/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/data/curated/yellow/_temporary/0/_temporary/attempt_20220801172202464652039855278631_0007_m_000011_25\n",
      "22/08/01 17:23:16 ERROR FileFormatWriter: Job job_20220801172202464652039855278631_0007 aborted.\n",
      "22/08/01 17:23:16 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2498/1068316462.py\", line 24, in <cell line: 24>\n",
      "    sdf_y.write.mode(\"overwrite\").parquet(\"../data/curated/yellow\")\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\", line 1140, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/andrew/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/mnt/c/Users/andre/Downloads/Unimelb/MAST30034 Applied Data Science/Project 1/GitHub/notebooks/preprocess.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000007vscode-remote?line=21'>22</a>\u001b[0m sdf_y \u001b[39m=\u001b[39m sdf_y\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mlpep_dropoff_datetime\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mlpep_pickup_datetime\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/andre/Downloads/Unimelb/MAST30034%20Applied%20Data%20Science/Project%201/GitHub/notebooks/preprocess.ipynb#ch0000007vscode-remote?line=23'>24</a>\u001b[0m sdf_y\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39m../data/curated/yellow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2004\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2002\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2004\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2005\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2006\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    532\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    533\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    535\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    539\u001b[0m }\n\u001b[1;32m    541\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    542\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# the yellow taxi data does not have \"trip_type\", but it has \"airport_fee\", \n",
    "# so the total number of columns actually ends up being the same anyway\n",
    "feats[12] = \"tpep_pickup_datetime\"\n",
    "feats[13] = \"tpep_dropoff_datetime\"\n",
    "time_diff = unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")\n",
    "sdf_y1 = sdf_y1.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_y1 = sdf_y1.withColumn(\"trip_duration\", time_diff)\n",
    "sdf_y1 = sdf_y1.drop(\"tpep_dropoff_datetime\").drop(\"tpep_pickup_datetime\")\n",
    "\n",
    "sdf_y2 = sdf_y2.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_y3 = sdf_y3.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_y4 = sdf_y4.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_y5 = sdf_y5.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_y6 = sdf_y6.select(col(feats[1]), col(feats[2]), col(feats[3]), col(feats[4]), col(feats[5]), col(feats[6]), \\\n",
    "    col(feats[7]), col(feats[8]), col(feats[9]), col(feats[10]), col(\"airport_fee\"), col(feats[11]), col(feats[12]), col(feats[13]))\n",
    "sdf_y = sdf_y1.union(sdf_y2).union(sdf_y3).union(sdf_y4).union(sdf_y5).union(sdf_y6)\n",
    "\n",
    "\n",
    "sdf_y.write.mode(\"overwrite\").parquet(\"../data/curated/yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting repetitive, but, I still need to do it again for HVFV vehicles. (RIP my storage space)\n",
    "\n",
    "#### NOTE: I will not be using fhvhv data anymore. These chunks of code from here all the way until the bottom will be soon deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17762 entries, 0 to 17761\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   hvfhs_license_num     17762 non-null  object        \n",
      " 1   dispatching_base_num  17762 non-null  object        \n",
      " 2   originating_base_num  12989 non-null  object        \n",
      " 3   request_datetime      17762 non-null  datetime64[ns]\n",
      " 4   on_scene_datetime     12989 non-null  datetime64[ns]\n",
      " 5   pickup_datetime       17762 non-null  datetime64[ns]\n",
      " 6   dropoff_datetime      17762 non-null  datetime64[ns]\n",
      " 7   PULocationID          17762 non-null  int64         \n",
      " 8   DOLocationID          17762 non-null  int64         \n",
      " 9   trip_miles            17762 non-null  float64       \n",
      " 10  trip_time             17762 non-null  int64         \n",
      " 11  base_passenger_fare   17762 non-null  float64       \n",
      " 12  tolls                 17762 non-null  float64       \n",
      " 13  bcf                   17762 non-null  float64       \n",
      " 14  sales_tax             17762 non-null  float64       \n",
      " 15  congestion_surcharge  17762 non-null  float64       \n",
      " 16  airport_fee           17762 non-null  float64       \n",
      " 17  tips                  17762 non-null  float64       \n",
      " 18  driver_pay            17762 non-null  float64       \n",
      " 19  shared_request_flag   17762 non-null  object        \n",
      " 20  shared_match_flag     17762 non-null  object        \n",
      " 21  access_a_ride_flag    17762 non-null  object        \n",
      " 22  wav_request_flag      17762 non-null  object        \n",
      " 23  wav_match_flag        17762 non-null  object        \n",
      "dtypes: datetime64[ns](4), float64(9), int64(3), object(8)\n",
      "memory usage: 3.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2022-04-09 03:41:29</td>\n",
       "      <td>2022-04-09 03:47:46</td>\n",
       "      <td>2022-04-09 03:48:29</td>\n",
       "      <td>2022-04-09 04:09:27</td>\n",
       "      <td>94</td>\n",
       "      <td>147</td>\n",
       "      <td>3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.22</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13949</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2022-04-25 01:33:48</td>\n",
       "      <td>2022-04-25 01:38:58</td>\n",
       "      <td>2022-04-25 01:39:39</td>\n",
       "      <td>2022-04-25 01:59:36</td>\n",
       "      <td>164</td>\n",
       "      <td>66</td>\n",
       "      <td>5.97</td>\n",
       "      <td>...</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>17.48</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2022-04-05 09:02:59</td>\n",
       "      <td>2022-04-05 09:05:18</td>\n",
       "      <td>2022-04-05 09:06:46</td>\n",
       "      <td>2022-04-05 09:15:31</td>\n",
       "      <td>127</td>\n",
       "      <td>235</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.11</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B03406</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-05 09:29:14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-04-05 09:36:28</td>\n",
       "      <td>2022-04-05 09:50:16</td>\n",
       "      <td>181</td>\n",
       "      <td>225</td>\n",
       "      <td>3.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.72</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2022-04-15 03:43:05</td>\n",
       "      <td>2022-04-15 03:49:38</td>\n",
       "      <td>2022-04-15 03:49:43</td>\n",
       "      <td>2022-04-15 04:06:53</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>...</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.84</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2022-04-22 03:12:18</td>\n",
       "      <td>2022-04-22 03:19:29</td>\n",
       "      <td>2022-04-22 03:21:30</td>\n",
       "      <td>2022-04-22 03:39:36</td>\n",
       "      <td>162</td>\n",
       "      <td>230</td>\n",
       "      <td>1.19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>11.39</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "4592             HV0003               B03404               B03404   \n",
       "13949            HV0003               B03404               B03404   \n",
       "2458             HV0003               B03404               B03404   \n",
       "2460             HV0005               B03406                 None   \n",
       "8176             HV0003               B03404               B03404   \n",
       "12115            HV0003               B03404               B03404   \n",
       "\n",
       "         request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "4592  2022-04-09 03:41:29 2022-04-09 03:47:46 2022-04-09 03:48:29   \n",
       "13949 2022-04-25 01:33:48 2022-04-25 01:38:58 2022-04-25 01:39:39   \n",
       "2458  2022-04-05 09:02:59 2022-04-05 09:05:18 2022-04-05 09:06:46   \n",
       "2460  2022-04-05 09:29:14                 NaT 2022-04-05 09:36:28   \n",
       "8176  2022-04-15 03:43:05 2022-04-15 03:49:38 2022-04-15 03:49:43   \n",
       "12115 2022-04-22 03:12:18 2022-04-22 03:19:29 2022-04-22 03:21:30   \n",
       "\n",
       "         dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  \\\n",
       "4592  2022-04-09 04:09:27            94           147        3.25  ...   \n",
       "13949 2022-04-25 01:59:36           164            66        5.97  ...   \n",
       "2458  2022-04-05 09:15:31           127           235        1.20  ...   \n",
       "2460  2022-04-05 09:50:16           181           225        3.33  ...   \n",
       "8176  2022-04-15 04:06:53            22            22        2.38  ...   \n",
       "12115 2022-04-22 03:39:36           162           230        1.19  ...   \n",
       "\n",
       "       sales_tax  congestion_surcharge  airport_fee  tips  driver_pay  \\\n",
       "4592        1.81                  0.00          0.0  0.00       19.22   \n",
       "13949       2.45                  2.75          0.0  3.36       17.48   \n",
       "2458        0.70                  0.00          0.0  0.00        6.11   \n",
       "2460        1.19                  0.00          0.0  0.00       13.72   \n",
       "8176        1.49                  0.00          0.0  0.00       11.84   \n",
       "12115       1.27                  2.75          0.0  5.00       11.39   \n",
       "\n",
       "       shared_request_flag  shared_match_flag  access_a_ride_flag  \\\n",
       "4592                     N                  N                       \n",
       "13949                    N                  N                       \n",
       "2458                     N                  N                       \n",
       "2460                     N                  N                   N   \n",
       "8176                     N                  N                       \n",
       "12115                    N                  N                       \n",
       "\n",
       "       wav_request_flag wav_match_flag  \n",
       "4592                  N              N  \n",
       "13949                 N              N  \n",
       "2458                  N              N  \n",
       "2460                  N              N  \n",
       "8176                  N              N  \n",
       "12115                 N              N  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhv1 = read_parquet('../data/raw/fhvhv_tripdata_2021-11.parquet')\n",
    "sdf_fhv2 = read_parquet('../data/raw/fhvhv_tripdata_2021-12.parquet')\n",
    "sdf_fhv3 = read_parquet('../data/raw/fhvhv_tripdata_2022-01.parquet')\n",
    "sdf_fhv4 = read_parquet('../data/raw/fhvhv_tripdata_2022-02.parquet')\n",
    "sdf_fhv5 = read_parquet('../data/raw/fhvhv_tripdata_2022-03.parquet')\n",
    "sdf_fhv6 = read_parquet('../data/raw/fhvhv_tripdata_2022-04.parquet')\n",
    "df_fhv6 = sdf_fhv6.sample(0.001, seed=0).toPandas()\n",
    "df_fhv6.info()\n",
    "df_fhv6.sample(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>request_datetime</th><th>on_scene_datetime</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "|hvfhs_license_num|dispatching_base_num|originating_base_num|request_datetime|on_scene_datetime|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
       "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "+-----------------+--------------------+--------------------+----------------+-----------------+---------------+----------------+------------+------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhv1.where(sdf_fhv1.shared_match_flag != \"N\").where(sdf_fhv1.hvfhs_license_num != \"HV0005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on shared rides: \"shared_request_flag\" always shows \"N\". I don't know if this is an error from whoever originated the data or it's the way it is.\n",
    "\n",
    "\"shared_match_flag\", however, can show \"Y\" even if the request is \"N\". The interesting thing is, only Lyft (HV0005) can show \"Y\", while the rest of the license num always shows \"N\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>request_datetime</th><th>on_scene_datetime</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 12:40:37</td><td>null</td><td>2021-11-01 12:44:51</td><td>2021-11-01 13:09:46</td><td>79</td><td>235</td><td>12.78</td><td>1495</td><td>53.74</td><td>0.0</td><td>1.61</td><td>4.77</td><td>2.75</td><td>0.0</td><td>0.0</td><td>26.7</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 13:11:54</td><td>null</td><td>2021-11-01 13:14:08</td><td>2021-11-01 13:25:58</td><td>48</td><td>163</td><td>2.215</td><td>710</td><td>15.7</td><td>0.0</td><td>0.47</td><td>1.39</td><td>2.75</td><td>0.0</td><td>0.0</td><td>8.41</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 14:12:36</td><td>null</td><td>2021-11-01 14:17:09</td><td>2021-11-01 15:01:02</td><td>158</td><td>49</td><td>10.089</td><td>2658</td><td>121.24</td><td>0.0</td><td>3.64</td><td>10.76</td><td>2.75</td><td>0.0</td><td>0.0</td><td>47.53</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:09:31</td><td>null</td><td>2021-11-01 15:14:12</td><td>2021-11-01 15:35:05</td><td>162</td><td>265</td><td>6.742</td><td>1253</td><td>52.15</td><td>20.0</td><td>2.16</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>27.73</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:01:14</td><td>null</td><td>2021-11-01 15:06:33</td><td>2021-11-01 15:33:44</td><td>114</td><td>155</td><td>10.921</td><td>1631</td><td>89.63</td><td>2.17</td><td>2.75</td><td>8.15</td><td>2.75</td><td>0.0</td><td>0.0</td><td>40.09</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 14:54:38</td><td>null</td><td>2021-11-01 15:18:59</td><td>2021-11-01 15:40:15</td><td>148</td><td>262</td><td>6.458</td><td>1526</td><td>25.31</td><td>0.0</td><td>0.76</td><td>2.25</td><td>2.75</td><td>0.0</td><td>0.0</td><td>21.47</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:34:14</td><td>null</td><td>2021-11-01 15:37:54</td><td>2021-11-01 15:47:10</td><td>246</td><td>164</td><td>1.282</td><td>556</td><td>33.69</td><td>0.0</td><td>1.01</td><td>2.99</td><td>2.75</td><td>0.0</td><td>0.0</td><td>19.92</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:50:47</td><td>null</td><td>2021-11-01 15:53:34</td><td>2021-11-01 16:08:18</td><td>114</td><td>255</td><td>3.454</td><td>884</td><td>39.77</td><td>0.0</td><td>1.19</td><td>3.53</td><td>2.75</td><td>0.0</td><td>0.0</td><td>18.46</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:24:16</td><td>null</td><td>2021-11-01 15:32:17</td><td>2021-11-01 15:38:19</td><td>241</td><td>127</td><td>1.623</td><td>436</td><td>20.67</td><td>0.0</td><td>0.62</td><td>1.83</td><td>0.0</td><td>0.0</td><td>0.0</td><td>10.45</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:09:35</td><td>null</td><td>2021-11-01 15:14:53</td><td>2021-11-01 15:32:53</td><td>198</td><td>33</td><td>5.688</td><td>1111</td><td>44.14</td><td>0.0</td><td>1.32</td><td>3.92</td><td>0.0</td><td>0.0</td><td>0.0</td><td>24.01</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:23:10</td><td>null</td><td>2021-11-01 15:25:01</td><td>2021-11-01 15:38:41</td><td>163</td><td>179</td><td>3.649</td><td>820</td><td>40.89</td><td>0.23</td><td>1.23</td><td>3.65</td><td>2.75</td><td>0.0</td><td>0.0</td><td>18.92</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:52:47</td><td>null</td><td>2021-11-01 16:00:33</td><td>2021-11-01 16:16:42</td><td>256</td><td>56</td><td>8.008</td><td>1015</td><td>53.4</td><td>0.0</td><td>1.6</td><td>4.74</td><td>0.0</td><td>0.0</td><td>0.0</td><td>22.99</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 16:33:13</td><td>null</td><td>2021-11-01 16:36:40</td><td>2021-11-01 16:52:28</td><td>203</td><td>132</td><td>6.68</td><td>948</td><td>23.85</td><td>0.0</td><td>0.79</td><td>2.34</td><td>0.0</td><td>2.5</td><td>0.0</td><td>15.36</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 15:57:08</td><td>null</td><td>2021-11-01 16:06:15</td><td>2021-11-01 16:12:22</td><td>230</td><td>164</td><td>1.328</td><td>367</td><td>25.03</td><td>0.0</td><td>0.75</td><td>2.22</td><td>2.75</td><td>0.0</td><td>0.0</td><td>9.51</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 16:01:12</td><td>null</td><td>2021-11-01 16:06:52</td><td>2021-11-01 16:31:53</td><td>188</td><td>138</td><td>13.269</td><td>1662</td><td>78.08</td><td>0.0</td><td>2.34</td><td>6.93</td><td>0.0</td><td>0.0</td><td>0.0</td><td>36.15</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 16:01:30</td><td>null</td><td>2021-11-01 16:09:38</td><td>2021-11-01 16:31:43</td><td>68</td><td>138</td><td>9.937</td><td>1371</td><td>54.02</td><td>5.99</td><td>1.88</td><td>5.55</td><td>2.75</td><td>2.5</td><td>14.54</td><td>27.2</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 17:48:30</td><td>null</td><td>2021-11-01 17:59:09</td><td>2021-11-01 18:18:42</td><td>79</td><td>138</td><td>10.271</td><td>1378</td><td>85.48</td><td>0.21</td><td>2.65</td><td>7.83</td><td>2.75</td><td>2.5</td><td>0.0</td><td>37.51</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 17:46:31</td><td>null</td><td>2021-11-01 17:51:42</td><td>2021-11-01 18:18:47</td><td>65</td><td>29</td><td>12.885</td><td>1625</td><td>92.15</td><td>0.0</td><td>2.76</td><td>8.18</td><td>0.0</td><td>0.0</td><td>0.0</td><td>42.73</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 17:48:44</td><td>null</td><td>2021-11-01 17:56:58</td><td>2021-11-01 18:10:39</td><td>51</td><td>259</td><td>2.692</td><td>821</td><td>21.97</td><td>0.0</td><td>0.66</td><td>1.95</td><td>0.0</td><td>0.0</td><td>0.0</td><td>9.86</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B03406</td><td>null</td><td>2021-11-01 17:48:39</td><td>null</td><td>2021-11-01 17:54:57</td><td>2021-11-01 18:03:31</td><td>125</td><td>234</td><td>1.861</td><td>617</td><td>38.11</td><td>0.0</td><td>1.14</td><td>3.38</td><td>2.75</td><td>0.0</td><td>0.0</td><td>18.42</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+--------------------+-------------------+-----------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee| tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
       "+-----------------+--------------------+--------------------+-------------------+-----------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "|           HV0005|              B03406|                null|2021-11-01 12:40:37|             null|2021-11-01 12:44:51|2021-11-01 13:09:46|          79|         235|     12.78|     1495|              53.74|  0.0|1.61|     4.77|                2.75|        0.0|  0.0|      26.7|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 13:11:54|             null|2021-11-01 13:14:08|2021-11-01 13:25:58|          48|         163|     2.215|      710|               15.7|  0.0|0.47|     1.39|                2.75|        0.0|  0.0|      8.41|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 14:12:36|             null|2021-11-01 14:17:09|2021-11-01 15:01:02|         158|          49|    10.089|     2658|             121.24|  0.0|3.64|    10.76|                2.75|        0.0|  0.0|     47.53|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:09:31|             null|2021-11-01 15:14:12|2021-11-01 15:35:05|         162|         265|     6.742|     1253|              52.15| 20.0|2.16|      0.0|                 0.0|        0.0|  0.0|     27.73|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:01:14|             null|2021-11-01 15:06:33|2021-11-01 15:33:44|         114|         155|    10.921|     1631|              89.63| 2.17|2.75|     8.15|                2.75|        0.0|  0.0|     40.09|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 14:54:38|             null|2021-11-01 15:18:59|2021-11-01 15:40:15|         148|         262|     6.458|     1526|              25.31|  0.0|0.76|     2.25|                2.75|        0.0|  0.0|     21.47|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:34:14|             null|2021-11-01 15:37:54|2021-11-01 15:47:10|         246|         164|     1.282|      556|              33.69|  0.0|1.01|     2.99|                2.75|        0.0|  0.0|     19.92|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:50:47|             null|2021-11-01 15:53:34|2021-11-01 16:08:18|         114|         255|     3.454|      884|              39.77|  0.0|1.19|     3.53|                2.75|        0.0|  0.0|     18.46|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:24:16|             null|2021-11-01 15:32:17|2021-11-01 15:38:19|         241|         127|     1.623|      436|              20.67|  0.0|0.62|     1.83|                 0.0|        0.0|  0.0|     10.45|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:09:35|             null|2021-11-01 15:14:53|2021-11-01 15:32:53|         198|          33|     5.688|     1111|              44.14|  0.0|1.32|     3.92|                 0.0|        0.0|  0.0|     24.01|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:23:10|             null|2021-11-01 15:25:01|2021-11-01 15:38:41|         163|         179|     3.649|      820|              40.89| 0.23|1.23|     3.65|                2.75|        0.0|  0.0|     18.92|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:52:47|             null|2021-11-01 16:00:33|2021-11-01 16:16:42|         256|          56|     8.008|     1015|               53.4|  0.0| 1.6|     4.74|                 0.0|        0.0|  0.0|     22.99|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 16:33:13|             null|2021-11-01 16:36:40|2021-11-01 16:52:28|         203|         132|      6.68|      948|              23.85|  0.0|0.79|     2.34|                 0.0|        2.5|  0.0|     15.36|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 15:57:08|             null|2021-11-01 16:06:15|2021-11-01 16:12:22|         230|         164|     1.328|      367|              25.03|  0.0|0.75|     2.22|                2.75|        0.0|  0.0|      9.51|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 16:01:12|             null|2021-11-01 16:06:52|2021-11-01 16:31:53|         188|         138|    13.269|     1662|              78.08|  0.0|2.34|     6.93|                 0.0|        0.0|  0.0|     36.15|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 16:01:30|             null|2021-11-01 16:09:38|2021-11-01 16:31:43|          68|         138|     9.937|     1371|              54.02| 5.99|1.88|     5.55|                2.75|        2.5|14.54|      27.2|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 17:48:30|             null|2021-11-01 17:59:09|2021-11-01 18:18:42|          79|         138|    10.271|     1378|              85.48| 0.21|2.65|     7.83|                2.75|        2.5|  0.0|     37.51|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 17:46:31|             null|2021-11-01 17:51:42|2021-11-01 18:18:47|          65|          29|    12.885|     1625|              92.15|  0.0|2.76|     8.18|                 0.0|        0.0|  0.0|     42.73|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 17:48:44|             null|2021-11-01 17:56:58|2021-11-01 18:10:39|          51|         259|     2.692|      821|              21.97|  0.0|0.66|     1.95|                 0.0|        0.0|  0.0|      9.86|                  N|                Y|                 N|               N|             N|\n",
       "|           HV0005|              B03406|                null|2021-11-01 17:48:39|             null|2021-11-01 17:54:57|2021-11-01 18:03:31|         125|         234|     1.861|      617|              38.11|  0.0|1.14|     3.38|                2.75|        0.0|  0.0|     18.42|                  N|                Y|                 N|               N|             N|\n",
       "+-----------------+--------------------+--------------------+-------------------+-----------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fhv1.where(sdf_fhv1.shared_match_flag != \"N\").where(sdf_fhv1.hvfhs_license_num == \"HV0005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know I only showed about 1/6 of the data here, but I tested on all of them and the result is consistent. As a result, I will ignore the \"shared ride\" columns due to the inconsistency of the data. Since I ignore said columns, I will also not include \"Lyft\" as part of my curated data, since they are the ones who have differing values in terms of shared rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sadly, I can't merge all these together because my device can't handle the size limit.\n",
    "\n",
    "feats_fhv = [\"hvfhs_license_num\", \"trip_miles\", \"trip_time\", \"PULocationID\", \"DOLocationID\", \"base_passenger_fare\", \\\n",
    "    \"tolls\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\", \"tips\", \"driver_pay\"]\n",
    "\n",
    "sdf_fhv1 = sdf_fhv1.where(sdf_fhv1.hvfhs_license_num != \"HV0005\")\n",
    "sdf_fhv2 = sdf_fhv2.where(sdf_fhv2.hvfhs_license_num != \"HV0005\")\n",
    "sdf_fhv3 = sdf_fhv3.where(sdf_fhv3.hvfhs_license_num != \"HV0005\")\n",
    "sdf_fhv4 = sdf_fhv4.where(sdf_fhv4.hvfhs_license_num != \"HV0005\")\n",
    "sdf_fhv5 = sdf_fhv5.where(sdf_fhv5.hvfhs_license_num != \"HV0005\")\n",
    "sdf_fhv6 = sdf_fhv6.where(sdf_fhv6.hvfhs_license_num != \"HV0005\")\n",
    "\n",
    "sdf_fhv1 = sdf_fhv1.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n",
    "sdf_fhv2 = sdf_fhv2.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n",
    "sdf_fhv3 = sdf_fhv3.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n",
    "sdf_fhv4 = sdf_fhv4.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n",
    "sdf_fhv5 = sdf_fhv5.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n",
    "sdf_fhv6 = sdf_fhv6.select(col(feats_fhv[0]), col(feats_fhv[1]), col(feats_fhv[2]), col(feats_fhv[3]), col(feats_fhv[4]), col(feats_fhv[5]), \\\n",
    "    col(feats_fhv[6]), col(feats_fhv[7]), col(feats_fhv[8]), col(feats_fhv[9]), col(feats_fhv[10]), col(feats_fhv[11]), col(feats_fhv[12]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_fhv1.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv1\")\n",
    "sdf_fhv2.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv2\")\n",
    "sdf_fhv3.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv3\")\n",
    "sdf_fhv4.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv4\")\n",
    "sdf_fhv5.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv5\")\n",
    "sdf_fhv6.write.mode(\"overwrite\").parquet(\"../data/curated/fhvhv6\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yay! all done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
